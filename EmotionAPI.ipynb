{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion API Python Quickstart\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Video API Preview ended on October 30, 2017. Try the new [Video Indexer API Preview](https://azure.microsoft.com/services/cognitive-services/video-indexer/) to easily extract insights from \n",
    "videos and to enhance content discovery experiences, such as search results, by detecting spoken words, faces, characters, and emotions. [Learn more](https://docs.microsoft.com/azure/cognitive-services/video-indexer/video-indexer-overview).\n",
    "\n",
    "This walkthrough provides information and code samples to help you quickly get started using the [Emotion API Recognize method](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89/operations/563b31ea778daf121cc3a5fa) with Python to recognize the emotions expressed by one or more people in an image. \n",
    "\n",
    "You can run this example as a Jupyter notebook on [MyBinder](https://mybinder.org) by clicking on the launch Binder badge: \n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/Microsoft/cognitive-services-notebooks/master?filepath=EmotionAPI.ipynb)\n",
    "\n",
    "\n",
    "## Prerequisite\n",
    "Get your free Subscription Key [here](https://azure.microsoft.com/try/cognitive-services/)\n",
    "\n",
    "## Running the walkthrough\n",
    "To continue with this walkthrough, replace `subscription_key` with the API key you obtained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscription_key = None\n",
    "assert subscription_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, verify that the service URL corresponds to the region you used when setting up the API key. If you are using a trial key, you do not need to make any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_recognition_url = \"https://westus.api.cognitive.microsoft.com/face/v1.0/detect?returnFaceId=true&returnFaceLandmarks=false&returnFaceAttributes=emotion
\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This walkthrough uses images that are stored on disk. You can also use images that are available via a publically accessible URL. For more information, see the [REST API documentation](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89/operations/563b31ea778daf121cc3a5fa).\n",
    "\n",
    "Since the image data is passed as part of the request body, notice that you need to set the `Content-Type` header to `application/octet-stream`. If you are passing in an image via a URL, remember to set the header to:\n",
    "```python\n",
    "header = {'Ocp-Apim-Subscription-Key': subscription_key }\n",
    "```\n",
    "create a dictionary containing the URL:\n",
    "```python\n",
    "data = {'url': image_url}\n",
    "```\n",
    "and pass that to the `requests` library using:\n",
    "```python\n",
    "requests.post(emotion_recognition_url, headers=headers, json=image_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download a few sample images from the [Emotion API](https://azure.microsoft.com/services/cognitive-services/emotion/) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p images\n",
    "curl -Ls https://aka.ms/csnb-emotion-1 -o images/emotion_1.jpg\n",
    "curl -Ls https://aka.ms/csnb-emotion-2 -o images/emotion_2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path = \"images/emotion_1.jpg\"\n",
    "image_data = open(image_path, \"rb\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'faceRectangle': {'height': 162, 'left': 130, 'top': 141, 'width': 162},\n",
       "  'scores': {'anger': 9.29041e-06,\n",
       "   'contempt': 0.000118981574,\n",
       "   'disgust': 3.15619363e-05,\n",
       "   'fear': 0.000589638,\n",
       "   'happiness': 0.06630674,\n",
       "   'neutral': 0.00555004273,\n",
       "   'sadness': 7.44669524e-06,\n",
       "   'surprise': 0.9273863}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key, \"Content-Type\": \"application/octet-stream\" }\n",
    "response = requests.post(emotion_recognition_url, headers=headers, data=image_data)\n",
    "response.raise_for_status()\n",
    "analysis = response.json()\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned JSON object contains the bounding boxes of the faces that were recognized along with the detected emotions. Each emotion is associated with a score between 0 and 1 where a higher score is more indicative of an emotion than a lower score. \n",
    "\n",
    "The following lines of code the detected emotions on the faces in the image using the `matplotlib` library. To reduce clutter, only the top three emotions are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "image  = Image.open(BytesIO(image_data))\n",
    "ax     = plt.imshow(image, alpha=0.6)\n",
    "\n",
    "for face in analysis:\n",
    "    fr = face[\"faceRectangle\"]\n",
    "    em = face[\"scores\"]\n",
    "    origin = (fr[\"left\"], fr[\"top\"])\n",
    "    p = Rectangle(origin, fr[\"width\"], fr[\"height\"], fill=False, linewidth=2, color='b')\n",
    "    ax.axes.add_patch(p)\n",
    "    ct = \"\\n\".join([\"{0:<10s}{1:>.4f}\".format(k,v) for k, v in sorted(list(em.items()),key=lambda r: r[1], reverse=True)][:3])\n",
    "    plt.text(origin[0], origin[1], ct, fontsize=20)    \n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `annotate_image` function shown next can be used to overlay emotions on top of an image file given its path on the file system. It is based on the code for calling into the Emotion API shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotate_image(image_path):    \n",
    "    image_data = open(image_path, \"rb\").read()\n",
    "    headers  = {'Ocp-Apim-Subscription-Key': subscription_key, \"Content-Type\": \"application/octet-stream\" }\n",
    "    response = requests.post(emotion_recognition_url, headers=headers, data=image_data)\n",
    "    response.raise_for_status()\n",
    "    analysis = response.json()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    image  = Image.open(image_path)\n",
    "    ax     = plt.imshow(image, alpha=0.6)\n",
    "\n",
    "    for face in analysis:\n",
    "        fr = face[\"faceRectangle\"]\n",
    "        em = face[\"scores\"]\n",
    "        origin = (fr[\"left\"], fr[\"top\"])\n",
    "        p = Rectangle(origin, fr[\"width\"], fr[\"height\"], fill=False, linewidth=2, color='b')\n",
    "        ax.axes.add_patch(p)\n",
    "        ct = \"\\n\".join([\"{0:<10s}{1:>.4f}\".format(k,v) for k, v in sorted(list(em.items()),key=lambda r: r[1], reverse=True)][:3])\n",
    "        plt.text(origin[0], origin[1], ct, fontsize=20, va=\"bottom\")    \n",
    "    _ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `annotate_image` function can be called on an image file as shown in the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotate_image(\"images/emotion_2.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "ms_docs_meta": {
   "author": "v-royhar",
   "description": "Get information and code samples to help you quickly get started using the Emotion API with Python in Cognitive Services.",
   "manager": "yutkuo",
   "ms.author": "anroth",
   "ms.date": "02/05/2018",
   "ms.service": "cognitive-services",
   "ms.technology": "emotion",
   "ms.topic": "article",
   "services": "cognitive-services",
   "title": "Emotion API Python quick start | Microsoft Docs"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
