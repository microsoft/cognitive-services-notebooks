{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion API Python Quick Start\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Video API Preview will end on October 30th, 2017. Try the new [Video Indexer API Preview](https://azure.microsoft.com/services/cognitive-services/video-indexer/) to easily extract insights from \n",
    "videos and to enhance content discovery experiences, such as search results, by detecting spoken words, faces, characters, and emotions. [Learn more](https://docs.microsoft.com/azure/cognitive-services/video-indexer/video-indexer-overview).\n",
    "\n",
    "This walkthrough provides information and code samples to help you quickly get started using the [Emotion API Recognize method](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89/operations/563b31ea778daf121cc3a5fa) with Python to recognize the emotions expressed by one or more people in an image. \n",
    "\n",
    "You can run this example as a Jupyter notebook on [MyBinder](https://mybinder.org) by clicking on the launch Binder badge: \n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/Microsoft/cognitive-services-notebooks/master?filepath=EmotionAPI.ipynb)\n",
    "\n",
    "\n",
    "## Prerequisite\n",
    "Get your free Subscription Key [here](https://azure.microsoft.com/en-us/try/cognitive-services/)\n",
    "\n",
    "## Running the walkthrough\n",
    "Please replace `subscription_key` below with the API key you obtained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscription_key = \"11966e80bb4a407bac5a340693a790e4\"\n",
    "assert subscription_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, please verify that the URL below corresponds to the region you used when setting up the API key. If you are using a trial key, you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_recognition_url = \"https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the emotion API walktrough, we will be using images that are stored on disk. You can also use images that are available via a publically-accessible URL. For more details, please see the [REST API](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89/operations/563b31ea778daf121cc3a5fa) documentation.\n",
    "\n",
    "Since the image data is passed as part of the request body, notice that we need to set the `Content-Type` header to `application/octet-stream`. If you are passing in an image via a URL, remember to set the header to:\n",
    "```python\n",
    "header = {'Ocp-Apim-Subscription-Key': subscription_key }\n",
    "```\n",
    "create a dictionary containing the URL:\n",
    "```python\n",
    "data = {'url': image_url}\n",
    "```\n",
    "and pass that to the `requests` library using:\n",
    "```python\n",
    "requests.post(emotion_recognition_url, headers=headers, json=image_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download a few sample images from the [Emotion API](https://azure.microsoft.com/en-us/services/cognitive-services/emotion/) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p images\n",
    "curl -Ls https://aka.ms/csnb-emotion-1 -o images/emotion_1.jpg\n",
    "curl -Ls https://aka.ms/csnb-emotion-2 -o images/emotion_2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path = \"images/emotion_1.jpg\"\n",
    "image_data = open(image_path, \"rb\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'faceRectangle': {'height': 162, 'left': 130, 'top': 141, 'width': 162},\n",
       "  'scores': {'anger': 9.29041e-06,\n",
       "   'contempt': 0.000118981574,\n",
       "   'disgust': 3.15619363e-05,\n",
       "   'fear': 0.000589638,\n",
       "   'happiness': 0.06630674,\n",
       "   'neutral': 0.00555004273,\n",
       "   'sadness': 7.44669524e-06,\n",
       "   'surprise': 0.9273863}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key, \"Content-Type\": \"application/octet-stream\" }\n",
    "response = requests.post(emotion_recognition_url, headers=headers, data=image_data)\n",
    "response.raise_for_status()\n",
    "analysis = response.json()\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned JSON object contains the bounding boxes of the faces that were recognized along with the detected emotions. Each emotion is associated with a score between $0$ and $1$ where a higher score is more indicative of an emotion than a lower score. \n",
    "\n",
    "We can now overlay the emotions on the faces in the image using `matplotlib` as shown below. To reduce clutter, we only show the top three emotions detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "image  = Image.open(BytesIO(image_data))\n",
    "ax     = plt.imshow(image, alpha=0.6)\n",
    "\n",
    "for face in analysis:\n",
    "    fr = face[\"faceRectangle\"]\n",
    "    em = face[\"scores\"]\n",
    "    origin = (fr[\"left\"], fr[\"top\"])\n",
    "    p = Rectangle(origin, fr[\"width\"], fr[\"height\"], fill=False, linewidth=2, color='b')\n",
    "    ax.axes.add_patch(p)\n",
    "    ct = \"\\n\".join([\"{0:<10s}{1:>.4f}\".format(k,v) for k, v in sorted(list(em.items()),key=lambda r: r[1], reverse=True)][:3])\n",
    "    plt.text(origin[0], origin[1], ct, fontsize=20)    \n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now generalize the function to overlay emotions on top of an image file given its path on the file system via the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotate_image(image_path):    \n",
    "    image_data = open(image_path, \"rb\").read()\n",
    "    headers  = {'Ocp-Apim-Subscription-Key': subscription_key, \"Content-Type\": \"application/octet-stream\" }\n",
    "    response = requests.post(emotion_recognition_url, headers=headers, data=image_data)\n",
    "    response.raise_for_status()\n",
    "    analysis = response.json()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    image  = Image.open(image_path)\n",
    "    ax     = plt.imshow(image, alpha=0.6)\n",
    "\n",
    "    for face in analysis:\n",
    "        fr = face[\"faceRectangle\"]\n",
    "        em = face[\"scores\"]\n",
    "        origin = (fr[\"left\"], fr[\"top\"])\n",
    "        p = Rectangle(origin, fr[\"width\"], fr[\"height\"], fill=False, linewidth=2, color='b')\n",
    "        ax.axes.add_patch(p)\n",
    "        ct = \"\\n\".join([\"{0:<10s}{1:>.4f}\".format(k,v) for k, v in sorted(list(em.items()),key=lambda r: r[1], reverse=True)][:3])\n",
    "        plt.text(origin[0], origin[1], ct, fontsize=20, va=\"bottom\")    \n",
    "    _ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_image(\"images/emotion_2.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "ms_docs_meta": {
   "author": "v-royhar",
   "description": "Get information and code samples to help you quickly get started using the Emotion API with Python in Cognitive Services.",
   "manager": "yutkuo",
   "ms.author": "anroth",
   "ms.date": "05/23/2017",
   "ms.service": "cognitive-services",
   "ms.technology": "emotion",
   "ms.topic": "article",
   "services": "cognitive-services",
   "title": "Emotion API Python quick start | Microsoft Docs"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
