{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart for Text Analytics API with Python \n",
    "<a name=\"HOLTop\"></a>\n",
    "\n",
    "Use this quickstart to begin analyzing language with the Text Analytics SDK for Python. [Text Analytics](//go.microsoft.com/fwlink/?LinkID=759711) REST API is compatible with most programming languages, the SDK provides an easy way to integrate the service into your applications.\n",
    "\n",
    "Refer to the [API definitions](//go.microsoft.com/fwlink/?LinkID=759346) for technical documentation for the APIs.\n",
    "\n",
    "You can run this example as a Jupyter notebook on [MyBinder](https://mybinder.org) by clicking on the launch Binder badge: \n",
    "\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/Microsoft/cognitive-services-notebooks/master?filepath=TextAnalytics.ipynb)\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You must have a [Cognitive Services API account](https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account) with **Text Analytics API**. You can use the **free tier for 5,000 transactions/month** to complete this walkthrough.\n",
    "\n",
    "You must also have the [endpoint and access key](../How-tos/text-analytics-how-to-access-key.md) that was generated for you during sign-up. \n",
    "\n",
    "To continue with this walkthrough, replace `subscription_key` with a valid subscription key that you obtained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "subscription_key = None\n",
    "assert subscription_key\n",
    "credentials = CognitiveServicesCredentials(subscription_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, verify that the region in `text_analytics_base_url` corresponds to the one you used when setting up the service. If you are using a free trial key, you do not need to change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analytics_url = \"https://westcentralus.api.cognitive.microsoft.com/\"\n",
    "text_analytics = TextAnalyticsClient(endpoint=text_analytics_url, credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"SentimentAnalysis\"></a>\n",
    "\n",
    "## Analyze sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of dictionaries, each representing a document you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  {\"id\": \"1\", \"language\": \"en\", \"text\": \"I had the best day of my life.\"},\n",
    "  {\"id\": \"2\", \"language\": \"en\", \"text\": \"This was a waste of my time. The speaker put me to sleep.\"},  \n",
    "  {\"id\": \"3\", \"language\": \"es\", \"text\": \"No tengo dinero ni nada que dar...\"},  \n",
    "  {\"id\": \"4\", \"language\": \"it\", \"text\": \"L'hotel veneziano era meraviglioso. È un bellissimo pezzo di architettura.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Call the `sentiment()` function and get the result. Then iterate through the results, and print each document's ID, and sentiment score. A score closer to 0 indicates a negative sentiment, while a score closer to 1 indicates a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Id:  1 , Sentiment Score:  0.87\n",
      "Document Id:  2 , Sentiment Score:  0.11\n",
      "Document Id:  3 , Sentiment Score:  0.44\n",
      "Document Id:  4 , Sentiment Score:  1.00\n"
     ]
    }
   ],
   "source": [
    "response = text_analytics.sentiment(documents=documents)\n",
    "for document in response.documents:\n",
    "     print(\"Document Id: \", document.id, \", Sentiment Score: \", \"{:.2f}\".format(document.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Detect\"></a>\n",
    "\n",
    "## Detect languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of dictionaries, containing the documents you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    { 'id': '1', 'text': 'This is a document written in English.' },\n",
    "    { 'id': '2', 'text': 'Este es un document escrito en Español.' },\n",
    "    { 'id': '3', 'text': '这是一个用中文写的文件' }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the client created earlier, call `detect_language()` function and get the result. Then iterate through the results, and print each document's ID, and the first returned language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Id:  1 , Language:  English\n",
      "Document Id:  2 , Language:  Spanish\n",
      "Document Id:  3 , Language:  Chinese_Simplified\n"
     ]
    }
   ],
   "source": [
    "response = text_analytics.detect_language(documents=documents)\n",
    "for document in response.documents:\n",
    "    print(\"Document Id: \", document.id , \", Language: \", document.detected_languages[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code render the response object as an HTML table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Text</th><th>Detected languages(scores)</th></tr><tr><td>This is a document written in English.</td><td>English(1.0)</td>\n",
       "<tr><td>Este es un document escrito en Español.</td><td>Spanish(1.0)</td>\n",
       "<tr><td>这是一个用中文写的文件</td><td>Chinese_Simplified(1.0)</td></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "table = []\n",
    "for document in response.documents:\n",
    "    text  = next(filter(lambda d: d[\"id\"] == document.id, documents))[\"text\"]\n",
    "    langs = \", \".join([\"{0}({1})\".format(lang.name, lang.score) for lang in document.detected_languages])\n",
    "    table.append(\"<tr><td>{0}</td><td>{1}</td>\".format(text, langs))\n",
    "HTML(\"<table><tr><th>Text</th><th>Detected languages(scores)</th></tr>{0}</table>\".format(\"\\n\".join(table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of dictionaries, containing the documents you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\"id\": \"1\",\"language\": \"en\", \"text\": \"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800.\"},\n",
    "    {\"id\": \"2\",\"language\": \"es\", \"text\": \"La sede principal de Microsoft se encuentra en la ciudad de Redmond, a 21 kilómetros de Seattle.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Using the client created earlier, call `entities()` function and get the result. Then iterate through the results, and print each document's ID, and the entities contained in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Id:  1\n",
      "\tKey Entities:\n",
      "\t\t NAME:  Microsoft \tType:  Organization \tSub-type:  None\n",
      "\t\t\tOffset:  0 \tLength:  9 \tScore:  1.00\n",
      "\t\t NAME:  Bill Gates \tType:  Person \tSub-type:  None\n",
      "\t\t\tOffset:  25 \tLength:  10 \tScore:  1.00\n",
      "\t\t NAME:  Paul Allen \tType:  Person \tSub-type:  None\n",
      "\t\t\tOffset:  40 \tLength:  10 \tScore:  1.00\n",
      "\t\t NAME:  April 4 \tType:  Other \tSub-type:  None\n",
      "\t\t\tOffset:  54 \tLength:  7 \tScore:  0.80\n",
      "\t\t NAME:  April 4, 1975 \tType:  DateTime \tSub-type:  Date\n",
      "\t\t\tOffset:  54 \tLength:  13 \tScore:  0.80\n",
      "\t\t NAME:  BASIC \tType:  Other \tSub-type:  None\n",
      "\t\t\tOffset:  89 \tLength:  5 \tScore:  0.80\n",
      "\t\t NAME:  Altair 8800 \tType:  Other \tSub-type:  None\n",
      "\t\t\tOffset:  116 \tLength:  11 \tScore:  0.80\n",
      "Document Id:  2\n",
      "\tKey Entities:\n",
      "\t\t NAME:  Microsoft \tType:  Organization \tSub-type:  None\n",
      "\t\t\tOffset:  21 \tLength:  9 \tScore:  1.00\n",
      "\t\t NAME:  Redmond (Washington) \tType:  Location \tSub-type:  None\n",
      "\t\t\tOffset:  60 \tLength:  7 \tScore:  0.99\n",
      "\t\t NAME:  21 kilómetros \tType:  Quantity \tSub-type:  Dimension\n",
      "\t\t\tOffset:  71 \tLength:  13 \tScore:  0.80\n",
      "\t\t NAME:  Seattle \tType:  Location \tSub-type:  None\n",
      "\t\t\tOffset:  88 \tLength:  7 \tScore:  1.00\n"
     ]
    }
   ],
   "source": [
    "response = text_analytics.entities(documents=documents)\n",
    "\n",
    "for document in response.documents:\n",
    "    print(\"Document Id: \", document.id)\n",
    "    print(\"\\tKey Entities:\")\n",
    "    for entity in document.entities:\n",
    "        print(\"\\t\\t\", \"NAME: \",entity.name, \"\\tType: \", entity.type, \"\\tSub-type: \", entity.sub_type)\n",
    "        for match in entity.matches:\n",
    "            print(\"\\t\\t\\tOffset: \", match.offset, \"\\tLength: \", match.length, \"\\tScore: \",\n",
    "                  \"{:.2f}\".format(match.entity_type_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"KeyPhraseExtraction\"></a>\n",
    "\n",
    "## Key phrase extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of dictionaries, containing the documents you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\"id\": \"1\", \"language\": \"ja\", \"text\": \"猫は幸せ\"},\n",
    "    {\"id\": \"2\", \"language\": \"de\", \"text\": \"Fahrt nach Stuttgart und dann zum Hotel zu Fu.\"},\n",
    "    {\"id\": \"3\", \"language\": \"en\", \"text\": \"My cat might need to see a veterinarian.\"},\n",
    "    {\"id\": \"4\", \"language\": \"es\", \"text\": \"A mi me encanta el fútbol!\"}\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the client created earlier, call `key_phrases()` function and get the result. Then iterate through the results, and print each document's ID, and the key phrases contained in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Id:  1\n",
      "\tPhrases:\n",
      "\t\t 幸せ\n",
      "Document Id:  2\n",
      "\tPhrases:\n",
      "\t\t Stuttgart\n",
      "\t\t Hotel\n",
      "\t\t Fahrt\n",
      "\t\t Fu\n",
      "Document Id:  3\n",
      "\tPhrases:\n",
      "\t\t cat\n",
      "\t\t veterinarian\n",
      "Document Id:  4\n",
      "\tPhrases:\n",
      "\t\t fútbol\n"
     ]
    }
   ],
   "source": [
    "response = text_analytics.key_phrases(documents=documents)\n",
    "\n",
    "for document in response.documents:\n",
    "    print(\"Document Id: \", document.id)\n",
    "    print(\"\\tKey Phrases:\")\n",
    "    for phrase in document.key_phrases:\n",
    "        print(\"\\t\\t\",phrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON object can once again be rendered as an HTML table using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Text</th><th>Key phrases</th></tr><tr><td>I had a wonderful experience! The rooms were wonderful and the staff was helpful.</td><td>wonderful experience,staff,rooms</td>\n",
       "<tr><td>I had a terrible time at the hotel. The staff was rude and the food was awful.</td><td>food,terrible time,hotel,staff</td>\n",
       "<tr><td>Los caminos que llevan hasta Monte Rainier son espectaculares y hermosos.</td><td>Monte Rainier,caminos</td>\n",
       "<tr><td>La carretera estaba atascada. Había mucho tráfico el día de ayer.</td><td>carretera,tráfico,día</td></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "table = []\n",
    "for document in response.documents:\n",
    "    text    = next(filter(lambda d: d[\"id\"] == document.id, documents))[\"text\"]    \n",
    "    phrases = \",\".join(document.key_phrases)\n",
    "    table.append(\"<tr><td>{0}</td><td>{1}</td>\".format(text, phrases))\n",
    "HTML(\"<table><tr><th>Text</th><th>Key phrases</th></tr>{0}</table>\".format(\"\\n\".join(table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "> [!div class=\"nextstepaction\"]\n",
    "> [Text Analytics With Power BI](../tutorials/tutorial-power-bi-key-phrases.md)\n",
    "\n",
    "## See also \n",
    "\n",
    " [Text Analytics overview](../overview.md)  \n",
    " [Frequently asked questions (FAQ)](../text-analytics-resource-faq.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonsdk",
   "language": "python",
   "name": "pythonsdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "ms_docs_meta": {
   "author": "luiscabrer",
   "description": "Get information and code samples to help you quickly get started using the Text Analytics API in Microsoft Cognitive Services on Azure.",
   "documentationcenter": "''",
   "ms.author": "luisca",
   "ms.date": "08/24/2017",
   "ms.service": "cognitive-services",
   "ms.technology": "text-analytics",
   "ms.topic": "article",
   "services": "cognitive-services",
   "title": "Python Quickstart for Azure Cognitive Services, Text Analytics API | Microsoft Docs"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
