{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Python Quick Starts\n",
    "\n",
    "This article provides information and code samples to help you quickly get started using the Computer Vision API with Python to accomplish the following tasks:\n",
    "* [Analyze an image](#AnalyzeImage)\n",
    "* [Use a domain-specific Model](#DomainSpecificModel)\n",
    "* [Intelligently generate a thumbnail](#GetThumbnail)\n",
    "* [Detect and extract printed text from an image](#OCR)\n",
    "* [Detect and extract handwritten text from an image](#RecognizeText)\n",
    "* [Analyze an image stored on disk](#AnalyzeImageOnDisk)\n",
    "\n",
    "To use the Computer Vision API, you need a subscription key. You can get free subscription keys [here](https://docs.microsoft.com/azure/cognitive-services/Computer-vision/Vision-API-How-to-Topics/HowToSubscribe).\n",
    "\n",
    "You can run this example as a Jupyter notebook on [MyBinder](https://mybinder.org) by clicking on the launch Binder badge: \n",
    "\n",
    "\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/Microsoft/cognitive-services-notebooks/master?filepath=VisionAPI.ipynb)\n",
    "\n",
    "\n",
    "## Analyze an image with Computer Vision API using Python \n",
    "<a name=\"AnalyzeImage\"> </a>\n",
    "\n",
    "With the [Analyze Image method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa), you can extract visual features based on image content. You can upload an image or specify an image URL and choose which features to return, including:\n",
    "* A detailed list of tags related to the image content.\n",
    "* A description of image content in a complete sentence.\n",
    "* The coordinates, gender, and age of any faces contained in the image.\n",
    "* The ImageType (clip art or a line drawing).\n",
    "* The dominant color, the accent color, or whether an image is black & white.\n",
    "* The category defined in this [taxonomy](https://docs.microsoft.com/azure/cognitive-services/computer-vision/category-taxonomy).\n",
    "* Does the image contain adult or sexually suggestive content?\n",
    "\n",
    "### Analyze an image \n",
    "To begin analyzing images, replace `subscription_key` with a valid API key that you obtained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "subscription_key = None\n",
    "assert subscription_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, ensure that region in `vision_base_url` corresponds to the one where you generated the API key (`westus`, `westcentralus`, etc.). If you are using a free trial subscription key, you do not need to make any changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "vision_base_url = \"https://westcentralus.api.cognitive.microsoft.com/vision/v2.0/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image analysis URL looks like the following (see REST API docs [here](https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa)):\n",
    "<code>\n",
    "https://[location].api.cognitive.microsoft.com/vision/v2.0/<b>analyze</b>[?visualFeatures][&details][&language]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "vision_analyze_url = vision_base_url + \"analyze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin analyzing an image, set `image_url` to the URL of any image that you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Broadway_and_Times_Square_by_night.jpg/450px-Broadway_and_Times_Square_by_night.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block uses the `requests` library in Python to call out to the Computer Vision `analyze` API and return the results as a JSON object. The API key is passed in via the `headers` dictionary and the types of features to recognize via the `params` dictionary. To see the full list of options that can be used, refer to the [REST API](https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa) documentation for image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "import requests\n",
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key }\n",
    "params   = {'visualFeatures': 'Categories,Description,Color'}\n",
    "data     = {'url': image_url}\n",
    "response = requests.post(vision_analyze_url, headers=headers, params=params, json=data)\n",
    "response.raise_for_status()\n",
    "analysis = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `analysis` object contains various fields that describe the image. The most relevant caption for the image can be obtained from the `descriptions` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()\n",
    "print(image_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code display the image and overlay it with the inferred caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "image = Image.open(BytesIO(requests.get(image_url).content))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.title(image_caption, size=\"x-large\", y=-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a domain-specific model <a name=\"DomainSpecificModel\"> </a>\n",
    "\n",
    "A [domain-specific model](https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fd)  is a model trained to identify a specific set of objects in an image.  The two domain-specific models that are currently available are _celebrities_ and _landmarks_. \n",
    "\n",
    "To view the list of domain-specific models supported, you can make the following request against the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "model_url = vision_base_url + \"models\"\n",
    "headers   = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "models    = requests.get(model_url, headers=headers).json()\n",
    "[model[\"name\"] for model in models[\"models\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmark identification\n",
    "To begin using the domain-specific model for landmarks, set `image_url` to point to an image to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/f6/Bunker_Hill_Monument_2005.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service endpoint to analyze images for landmarks can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "landmark_analyze_url = vision_base_url + \"models/landmarks/analyze\"\n",
    "print(landmark_analyze_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image in `image_url` can now be analyzed for any landmarks. The identified landmark is stored in `landmark_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "params   = {'model': 'landmarks'}\n",
    "data     = {'url': image_url}\n",
    "response = requests.post(landmark_analyze_url, headers=headers, params=params, json=data)\n",
    "response.raise_for_status()\n",
    "\n",
    "analysis      = response.json()\n",
    "assert analysis[\"result\"][\"landmarks\"] is not []\n",
    "\n",
    "landmark_name = analysis[\"result\"][\"landmarks\"][0][\"name\"].capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image = Image.open(BytesIO(requests.get(image_url).content))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.title(landmark_name, size=\"x-large\", y=-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Celebrity identification\n",
    "Along the same lines, the domain-specific model for identifying celebrities can be invoked as shown next. First set `image_url` to point to the image of a celebrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/d/d9/Bill_gates_portrait.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service endpoint for detecting celebrity images can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "celebrity_analyze_url = vision_base_url + \"models/celebrities/analyze\"\n",
    "print(celebrity_analyze_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the image in `image_url` can be analyzed for celebrities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "params   = {'model': 'celebrities'}\n",
    "data     = {'url': image_url}\n",
    "response = requests.post(celebrity_analyze_url, headers=headers, params=params, json=data)\n",
    "response.raise_for_status()\n",
    "\n",
    "analysis = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code extract the name and bounding box for one of the celebrities found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "assert analysis[\"result\"][\"celebrities\"] is not []\n",
    "celebrity_info = analysis[\"result\"][\"celebrities\"][0]\n",
    "celebrity_name = celebrity_info[\"name\"]\n",
    "celebrity_face = celebrity_info[\"faceRectangle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, this information can be overlaid on top of the original image using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "image  = Image.open(BytesIO(requests.get(image_url).content))\n",
    "ax     = plt.imshow(image, alpha=0.6)\n",
    "origin = (celebrity_face[\"left\"], celebrity_face[\"top\"])\n",
    "p      = Rectangle(origin, celebrity_face[\"width\"], celebrity_face[\"height\"], \n",
    "                   fill=False, linewidth=2, color='b')\n",
    "ax.axes.add_patch(p)\n",
    "plt.text(origin[0], origin[1], celebrity_name, fontsize=20, weight=\"bold\", va=\"bottom\")\n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a thumbnail with Computer Vision API\n",
    "<a name=\"GetThumbnail\"> </a>\n",
    "\n",
    "Use the [Get Thumbnail method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fb) to crop an image based on its region of interest (ROI) to the height and width you desire. The aspect ratio you set for the thumbnail can be different from the aspect ratio of the input image.\n",
    "\n",
    "To generate the thumbnail for an image, first set `image_url` to point to its location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/9/94/Bloodhound_Puppy.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service endpoint to generate the thumbnail can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "thumbnail_url = vision_base_url + \"generateThumbnail\"\n",
    "print(thumbnail_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a 50-by-50 pixel thumbnail for the image can be generated by calling this service endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "params   = {'width': '50', 'height': '50','smartCropping': 'true'}\n",
    "data     = {'url': image_url}\n",
    "response = requests.post(thumbnail_url, headers=headers, params=params, json=data)\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the thumbnail is indeed 50-by-50 pixels using the Python Image Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "thumbnail = Image.open(BytesIO(response.content))\n",
    "print(\"Thumbnail is {0}-by-{1}\".format(*thumbnail.size))\n",
    "thumbnail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical character recognition (OCR) with Computer Vision API <a name=\"OCR\"> </a>\n",
    "\n",
    "Use the [Optical Character Recognition (OCR) method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fc) to synchronously detect printed text in an image and extract recognized characters into a machine-usable character stream.\n",
    "\n",
    "To illustrate the OCR API, set `image_url` to point to the text to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Atomist_quote_from_Democritus.png/338px-Atomist_quote_from_Democritus.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service endpoint for OCR for your region can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "ocr_url = vision_base_url + \"ocr\"\n",
    "print(ocr_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can call into the OCR service to get the text that was recognized along with bounding boxes. In the parameters shown, `\"language\": \"unk\"` automatically detects the language in the text and `\"detectOrientation\": \"true\"` automatically aligns the image. For more information, see the [REST API documentation](https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "params   = {'language': 'unk', 'detectOrientation ': 'true'}\n",
    "data     = {'url': image_url}\n",
    "response = requests.post(ocr_url, headers=headers, params=params, json=data)\n",
    "response.raise_for_status()\n",
    "\n",
    "analysis = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word bounding boxes and text from the results of analysis can be extracted using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "line_infos = [region[\"lines\"] for region in analysis[\"regions\"]]\n",
    "word_infos = []\n",
    "for line in line_infos:\n",
    "    for word_metadata in line:\n",
    "        for word_info in word_metadata[\"words\"]:\n",
    "            word_infos.append(word_info)\n",
    "word_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the recognized text can be overlaid on top of the original image using the `matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "image  = Image.open(BytesIO(requests.get(image_url).content))\n",
    "ax     = plt.imshow(image, alpha=0.5)\n",
    "for word in word_infos:\n",
    "    bbox = [int(num) for num in word[\"boundingBox\"].split(\",\")]\n",
    "    text = word[\"text\"]\n",
    "    origin = (bbox[0], bbox[1])\n",
    "    patch  = Rectangle(origin, bbox[2], bbox[3], fill=False, linewidth=2, color='y')\n",
    "    ax.axes.add_patch(patch)\n",
    "    plt.text(origin[0], origin[1], text, fontsize=20, weight=\"bold\", va=\"top\")\n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text recognition with Computer Vision API <a name=\"RecognizeText\"> </a>\n",
    "\n",
    "Use the [Recognize Text method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2c6a154055056008f200) to asynchronously detect handwritten or printed text in an image and extract recognized characters into a machine-usable character stream.\n",
    "\n",
    "Set `image_url` to point to the image to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Cursive_Writing_on_Notebook_paper.jpg/800px-Cursive_Writing_on_Notebook_paper.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service endpoint for the text recognition service can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "text_recognition_url = vision_base_url + \"recognizeText\"\n",
    "print(text_recognition_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The handwritten text recognition service can be used to recognize the text in the image. In the `params` dictionary, you can set `mode` to `Printed` to recognize only printed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "params   = {'mode' : 'Handwritten'}\n",
    "data     = {'url': image_url}\n",
    "response = requests.post(text_recognition_url, headers=headers, params=params, json=data)\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text recognition service does not return the recognized text by itself. Instead, it returns immediately with an \"Operation Location\" URL in the response header that must be polled to get the result of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "operation_url = response.headers[\"Operation-Location\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the `operation_url`, you can query it for the analyzed text. The following lines of code implement a polling loop in order to wait for the operation to complete. Notice that the polling is done via an HTTP `GET` method instead of `POST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "analysis = {}\n",
    "while not \"recognitionResult\" in analysis:\n",
    "    response_final = requests.get(response.headers[\"Operation-Location\"], headers=headers)\n",
    "    analysis       = response_final.json()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the recognized text along with the bounding boxes can be extracted as shown in the following line of code. An important point to note is that the handwritten text recognition API returns bounding boxes as **polygons** instead of **rectangles**. Each polygon is _p_ is defined by its vertices specified using the following convention:\n",
    "\n",
    "<i>p</i> = [<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>, <i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>, ..., <i>x</i><sub>N</sub>, <i>y</i><sub>N</sub>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "polygons = [(line[\"boundingBox\"], line[\"text\"]) for line in analysis[\"recognitionResult\"][\"lines\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the recognized text can be overlaid on top of the original image using the extracted polygon information. Notice that `matplotlib` requires the vertices to be specified as a list of tuples of the form:\n",
    "\n",
    "<i>p</i> = [(<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>), (<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>), ..., (<i>x</i><sub>N</sub>, <i>y</i><sub>N</sub>)]\n",
    "\n",
    "and the post-processing code transforms the polygon data returned by the service into the form required by `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "image  = Image.open(BytesIO(requests.get(image_url).content))\n",
    "ax     = plt.imshow(image)\n",
    "for polygon in polygons:\n",
    "    vertices = [(polygon[0][i], polygon[0][i+1]) for i in range(0,len(polygon[0]),2)]\n",
    "    text     = polygon[1]\n",
    "    patch    = Polygon(vertices, closed=True,fill=False, linewidth=2, color='y')\n",
    "    ax.axes.add_patch(patch)\n",
    "    plt.text(vertices[0][0], vertices[0][1], text, fontsize=20, va=\"top\")\n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze an image stored on disk\n",
    "<a name=\"AnalyzeImageOnDisk\"> </a>\n",
    "The Computer Vision REST APIs don't just accept URLs to publically accessible images. They can also be provided the image to be analyzed as part of the HTTP body. For mode details of this feature, see the documentation [here](https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa). \n",
    "\n",
    "The code in this section uses this feature to analyze a sample image on disk. The primary difference between passing in an image URL vs. image data is that the header to the request must contain an entry of the form:\n",
    "```py\n",
    "{\"Content-Type\": \"application/octet-stream\"}\n",
    "```\n",
    "and the binary image data must be passed in via the `data` parameter to `requests.post` as opposed to the `json` parameter.\n",
    "\n",
    "First, download a sample image from the [Computer Vision API](https://azure.microsoft.com/services/cognitive-services/computer-vision/) page to the local file system and make `image_path` point to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p images\n",
    "curl -Ls https://aka.ms/csnb-house-yard -o images/house_yard.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_path = \"images/house_yard.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, read it into a byte array and send it to the Vision service to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "collapsed": true },
   "outputs": [],
   "source": [
    "image_data = open(image_path, \"rb\").read()\n",
    "headers    = {'Ocp-Apim-Subscription-Key': subscription_key, \n",
    "              \"Content-Type\": \"application/octet-stream\" }\n",
    "params     = {'visualFeatures': 'Categories,Description,Color'}\n",
    "response   = requests.post(vision_analyze_url, \n",
    "                           headers=headers, \n",
    "                           params=params, \n",
    "                           data=image_data)\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "analysis      = response.json()\n",
    "image_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()\n",
    "image_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the caption can be easily overlaid on the image. Notice that since the image is already available locally, the process is slightly shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.title(image_caption, size=\"x-large\", y=-0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "ms_docs_meta": {
   "author": "JuliaNik",
   "description": "Get information and code samples to help you quickly get started using Python and the Computer Vision API in Microsoft Cognitive Services.",
   "manager": "ytkuo",
   "ms.author": "juliakuz",
   "ms.date": "02/02/2018",
   "ms.service": "cognitive-services",
   "ms.technology": "computer-vision",
   "ms.topic": "article",
   "services": "cognitive-services",
   "title": "Computer Vision API Python quick start | Microsoft Docs",
   "titleSuffix": "Computer Vision API Python quick start | Microsoft Cognitive Services"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
